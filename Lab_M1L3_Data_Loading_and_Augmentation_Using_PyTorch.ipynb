{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Data Loading and Augmentation Using PyTorch</font></h1>\n",
    "\n",
    "<h1 align=left><font size = 5.5>Building efficient data loaders with PyTorch</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Estimated time: 60 minutes</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Objective</h2>\n",
    "After completing this lab, you'll be able to:\n",
    "\n",
    "- Load the data using Pytorch data loader\n",
    "- Augment the image dataset using PyTorch\n",
    "- Visualize the augmented data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to the PyTorch version of the data loading lab! You have explored the core concept of sequential loading by manually handling file paths and opening images one by one. This memory-efficient method is the foundation for how all modern deep learning frameworks handle large datasets.\n",
    "\n",
    "In this notebook, you will implement the **PyTorch** utilities for the same. You will discover PyTorch's modular approach, which separates *data representation* from *data iteration*:\n",
    "\n",
    "1.  **Custom `Dataset` Class:** You'll build our own dataset class from scratch by inheriting from `torch.utils.data.Dataset`. This gives you full control and a deep understanding of the data pipeline.\n",
    "2.  **In-built `ImageFolder` Utility:** You'll use the convenient `torchvision.datasets.ImageFolder` class, which automatically handles data from a standard directory structure.\n",
    "3.  **The `DataLoader`:** You'll wrap both of our datasets in a `DataLoader`, PyTorch's engine for efficient batching, shuffling, and parallelized data loading.\n",
    "\n",
    "Finally, you'll compare these methods to understand why this modular design is so powerful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "\n",
    "<div>\n",
    "<font size = 3>    \n",
    "\n",
    "0.  [Setup](#Setup)\n",
    "1.  [Custom PyTorch `Dataset`](#Custom-PyTorch-Dataset)\n",
    "2.  [In-built `ImageFolder` utility](#In-built-ImageFolder-utility)\n",
    "3.  [The `DataLoader` - Bringing it all together](#The-DataLoader---Bringing-it-all-together)\n",
    "4.  [Comparison and analysis](#Comparison-and-analysis)\n",
    "\n",
    "</font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's begin by importing our libraries and defining the data directories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required libraries\n",
    "\n",
    "Some of the required libraries are __not__ pre-installed in the Skills Network Labs environment. You must run the following __cell__ to install them; it might take a few minutes for the installation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 65.8 ms, sys: 36.6 ms, total: 102 ms\n",
      "Wall time: 17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install numpy==1.26\n",
    "%pip install matplotlib==3.9.2\n",
    "%pip install skillsnetwork\n",
    "%pip install pandas==2.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check if the above libraries are installed properly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Library installation was successful, let's proceed ahead\n"
     ]
    }
   ],
   "source": [
    "# define a function to check for successful installation of the libraries\n",
    "def lib_installation_check(captured_data, n_lines_print):\n",
    "    \"\"\"\n",
    "    A function to use the %%capture output from the cells where we try to install the libraries.\n",
    "    It would print last \"n_lines_print\" if there is an error in library installation\n",
    "    \"\"\"\n",
    "    output_text = captured_data.stdout\n",
    "    lines = output_text.splitlines()\n",
    "    output_last_n_lines = '\\n'.join(lines[-n_lines_print:])\n",
    "    if \"error\" in output_last_n_lines.lower():\n",
    "        print(\"❌ Library installation failed!\")\n",
    "        print(\"--- Error Details ---\")\n",
    "        print(output_last_n_lines)\n",
    "    else:\n",
    "        print(\"✅ Library installation was successful, let's proceed ahead\")    \n",
    "\n",
    "lib_installation_check(captured_data = captured_output, n_lines_print = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PyTorch (torch)` library installation\n",
    "\n",
    "Next, install the `PyTorch (torch)` library using the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.7.1\n",
      "  Downloading torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting filelock (from torch==2.7.1)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.1) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.7.1) (75.8.0)\n",
      "Collecting sympy>=1.13.3 (from torch==2.7.1)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.7.1)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.1) (3.1.5)\n",
      "Collecting fsspec (from torch==2.7.1)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.1)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.1)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.1)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.1)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.1)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.1)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.1)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.1)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.1)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.1)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.1)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.1)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.1)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.1)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch==2.7.1)\n",
      "  Downloading triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch==2.7.1)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.7.1) (3.0.2)\n",
      "Downloading torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl (821.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.0/821.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed filelock-3.19.1 fsspec-2025.9.0 mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.1 triton-3.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 5.79 s, sys: 1.28 s, total: 7.07 s\n",
      "Wall time: 5min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install torch==2.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torchvision` library installation\n",
    "\n",
    "Install the `torchvision` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision==0.22\n",
      "  Downloading torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision==0.22) (1.26.0)\n",
      "Collecting torch==2.7.0 (from torchvision==0.22)\n",
      "  Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision==0.22) (11.3.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (75.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (1.11.1.6)\n",
      "Collecting triton==3.3.0 (from torch==2.7.0->torchvision==0.22)\n",
      "  Downloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision==0.22) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.7.0->torchvision==0.22) (3.0.2)\n",
      "Downloading torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (865.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: triton, torch, torchvision\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.3.1\n",
      "    Uninstalling triton-3.3.1:\n",
      "      Successfully uninstalled triton-3.3.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.7.1\n",
      "    Uninstalling torch-2.7.1:\n",
      "      Successfully uninstalled torch-2.7.1\n",
      "Successfully installed torch-2.7.0 torchvision-0.22.0 triton-3.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 2.15 s, sys: 517 ms, total: 2.66 s\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install torchvision==0.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skillsnetwork\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "1. Download and extract data from the cloud using `skillsnetwork.prepare` method.\n",
    "2. Use a fallback method if the `skillsnetwork.prepare` command fails to download and extract the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up the data extraction directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_dir = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data acquisition and preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the dataset URL\n",
    "We define the `url` that holds the link to the dataset. The dataset is a `.tar` archive hosted on a cloud object storage service. Cloud object storage (such as S3) is a highly scalable and durable way to store and retrieve large amounts of unstructured data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "\n",
    "1. Download and extract data from the cloud using `skillsnetwork.prepare` method.\n",
    "2. Use a fallback method if the `skillsnetwork.prepare` command fails to download and extract the dataset. The fallback involves asynchronously downloading the `.tar` file using `httpx` and then extracting its contents using the `tarfile` library.\n",
    "3. The `tarfile` module provides an interface to tar archives, supporting various compression formats such as gzip and bzip2 (handled by `r:*` mode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\" function to check whether data download and extraction method \n",
    "    `skillsnetwork.prepare` would execute successfully, without downloading any data.\n",
    "    This helps in early detection and fast fallback to explicit download and extraction\n",
    "    using default libraries\n",
    "    ###This is a hack for the code to run on non-cloud computing environment without errors\n",
    "    \"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test) \n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "    os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"function to explicitly download and extract the dataset tar file from cloud using native python libraries\n",
    "    \"\"\"\n",
    "    if not os.path.exists(tar_path): # download only if file not downloaded already\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)# Download the file asynchronously\n",
    "                response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "            \n",
    "                with open(tar_path , \"wb\") as f:\n",
    "                    f.write(response.content) # Save the downloaded file\n",
    "                print(f\"Successfully downloaded '{file_name}'.\")\n",
    "        except httpx.HTTPStatusError as http_err:\n",
    "            print(f\"HTTP error occurred during download: {http_err}\")\n",
    "        except Exception as download_err:\n",
    "            print(f\"An error occurred during the fallback process: {download_err}\")\n",
    "    else:\n",
    "        print(f\"dataset tar file already downloaded at: {tar_path}\")\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "    print(f\"✅ Successfully extracted to '{extract_dir}'.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea82e84b38247c7a534dcb4c86d786d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f544c4791344c7594f0f78006c06187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    check_skillnetwork_extraction(extract_dir)\n",
    "    await skillsnetwork.prepare(url = url, path = extract_dir, overwrite = True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # --- FALLBACK METHOD FOR DOWNLOADING THE DATA ---\n",
    "    print(\"❌ Primary download/extration method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    \n",
    "    # import libraries required for downloading and extraction\n",
    "    import tarfile\n",
    "    import httpx \n",
    "    from pathlib import Path\n",
    "    \n",
    "    file_name = Path(url).name\n",
    "    tar_path = os.path.join(extract_dir, file_name)\n",
    "    print(f\"tar_path: {os.path.exists(tar_path)} ___ {tar_path}\")\n",
    "    await download_tar_dataset(url, tar_path, extract_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "base_dir = os.path.join(extract_dir, 'images_dataSAT')\n",
    "dir_non_agri = os.path.join(base_dir, 'class_0_non_agri')\n",
    "dir_agri = os.path.join(base_dir, 'class_1_agri')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom PyTorch `Dataset`\n",
    "\n",
    "In PyTorch, \n",
    "- A `Dataset` object is an iterable (gives one sample at a time).\n",
    "\n",
    "- A `DataLoader` is an iterator that returns batches (adds a batch dimension: `[B, C, H, W]`).\n",
    "\n",
    "- To create a custom dataset, we will create a class that inherits from `torch.utils.data.Dataset` and implements three essential methods:\n",
    "- `__init__(self)`: This method is run once when the dataset is instantiated. It's where you perform initial setup, such as loading file paths and labels.\n",
    "- `__len__(self)`: This returns the total number of samples in the dataset.\n",
    "- `__getitem__(self, idx)`: This method is responsible for loading and returning a *single* sample from the dataset at the given index `idx`. This is the core of sequential loading; it fetches one data point from storage \"just-in-time\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBinaryClassDataset(Dataset):\n",
    "    \"\"\"A custom dataset for our agricultural land classification task.\"\"\"\n",
    "    def __init__(self, non_agri_dir, agri_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            non_agri_dir (string): Directory with all the non-agricultural images.\n",
    "            agri_dir (string): Directory with all the agricultural images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load non-agri paths and assign label 0\n",
    "        for fname in os.listdir(non_agri_dir):\n",
    "            self.image_paths.append(os.path.join(non_agri_dir, fname))\n",
    "            self.labels.append(0)\n",
    "            \n",
    "        # Load agri paths and assign label 1\n",
    "        for fname in os.listdir(agri_dir):\n",
    "            self.image_paths.append(os.path.join(agri_dir, fname))\n",
    "            self.labels.append(1)\n",
    "\n",
    "        temp = list(zip(self.image_paths, self.labels))\n",
    "        np.random.shuffle(temp)\n",
    "        self.image_paths, self.labels = zip(*temp)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\") # Ensure image is in RGB format\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply transformations if they exist\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define transformations\n",
    "\n",
    "- `torchvision.transforms` provides a suite of common image transformations. These are used for preprocessing (such as `resizing` and `converting to a tensor`) and data augmentation (such as `random flips` and `rotations`).\n",
    "- `transforms.Compose` chains these operations together into a single, callable pipeline. Data augmentation is a crucial regularization technique that helps prevent overfitting by showing the model slightly different versions of the same image at each epoch.\n",
    "- Transforms like `RandomFlip` and `Rotation` work on PIL images and must be applied before `ToTensor()`, which converts images to tensors.\n",
    "- `Normalize()` scales tensor values (e.g., from `[0, 1]` to `[-1, 1]`) using `(x - mean) / std`  or can be based on model you are fine tuining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.RandomRotation(20),\n",
    "                                     transforms.ToTensor(), # Converts PIL Image to a tensor and scales values to [0, 1]\n",
    "                                     transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5]) # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how a data augmentation pipeline works in PyTorch, let’s create a transformation pipeline `custom_transform`for the given specifications. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 1**: Create a transformation pipeline `custom_transform` for: \n",
    "    1. image size = 64 x 64 pixels\n",
    "    2. RandomHorizontalFlip probability 0.5\n",
    "    3. RandomVerticalFlip probability 0.2\n",
    "    4. RandomRotation of 45 degrees\n",
    "\n",
    "Then, transform it to tensor and normalize it to [-1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n",
    "custom_transform = transforms.Compose([transforms.Resize((64, 64)),\n",
    "                                       transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                       transforms.RandomVerticalFlip(p=0.2),\n",
    "                                       transforms.RandomRotation(45),\n",
    "                                       transforms.ToTensor(), \n",
    "                                       transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # Normalize to [-1, 1]\n",
    "                                      ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\n",
    "custom_transform = transforms.Compose([transforms.Resize((64, 64)),\n",
    "                                       transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                       transforms.RandomVerticalFlip(p=0.2),\n",
    "                                       transforms.RandomRotation(45),\n",
    "                                       transforms.ToTensor(), \n",
    "                                       transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # Normalize to [-1, 1]\n",
    "                                      ])\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-built `ImageFolder` utility\n",
    "\n",
    "For datasets that follow a standard `root/class_name/xxx.ext` directory structure, PyTorch provides `torchvision.datasets.ImageFolder`. This is a specialized `Dataset` class that does all the work of finding image paths and inferring class labels for you. It's the most convenient way to load image datasets organized in folders, where each subfolder represents a different class.\n",
    "\n",
    "**ImageFolder** acts as a quick bridge between raw image files and PyTorch models. You can point the `root` argument at the top-level dataset folder, and the class automatically walks through every subfolder, using each subfolder’s name as a label. You can attach a `transform pipeline` with common choices including `transforms.Resize`, `transforms.CenterCrop`, `transforms.ToTensor`, and `transforms.Normalize` to pre-process every image on the fly. Calling `len(dataset)` reveals total images, while indexing `dataset[i]` returns an `(image, label)` pair. You can then combine it with `DataLoader` to batch, shuffle, and parallel-load data effortlessly.\n",
    "\n",
    "**ImageFolder** works on the `dataset folder` structured as represented below:\n",
    "\n",
    "    dataset/\n",
    "    ├── class1/\n",
    "    │   ├── image1.jpg\n",
    "    │   └── image2.jpg\n",
    "    └── class2/\n",
    "        ├── image3.jpg\n",
    "        └── image4.jpg\n",
    "    \n",
    "\n",
    "From this dataset, you can create a `dataset object` like this: \n",
    "\n",
    "```\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(\n",
    "    root='path/to/dataset',\n",
    "    transform=transform\n",
    "\n",
    "```\n",
    "\n",
    "Then, you can use this `dataset object` to create a `dataloader` like this:\n",
    "```\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for images, labels in dataloader:\n",
    "    # Your training code here\n",
    "    .\n",
    "    .\n",
    "    ...\n",
    "\n",
    "```\n",
    "This `dataloader` can then be used for creating batches of images for training / testing of the model.\n",
    "\n",
    "**`ImageFolder`** automatically assigns numerical labels to classes based on folder names alphabetically. Access class names via `dataset.classes` and class-to-index mapping via `dataset.class_to_idx`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to get the dataset and apply custom_transform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 2**: Get the dataset **imagefolder_dataset** using `datasets.Imagefolder` method and applying **custom_transform**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "imagefolder_dataset = datasets.ImageFolder(root=base_dir, transform=custom_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "imagefolder_dataset = datasets.ImageFolder(root=base_dir, transform=custom_transform)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, print the name and the class index. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 3**: Print the name and the class index from the **imagefolder_dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found by ImageFolder: ['class_0_non_agri', 'class_1_agri']\n",
      "Class to index mapping: {'class_0_non_agri': 0, 'class_1_agri': 1}\n"
     ]
    }
   ],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n",
    "print(f\"Classes found by ImageFolder: {imagefolder_dataset.classes}\")\n",
    "print(f\"Class to index mapping: {imagefolder_dataset.class_to_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "print(f\"Classes found by ImageFolder: {imagefolder_dataset.classes}\")\n",
    "print(f\"Class to index mapping: {imagefolder_dataset.class_to_idx}\")\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `DataLoader` - Bringing it all together\n",
    "\n",
    "The `DataLoader` is an iterator that returns batches, based on the `batch_size`. It wraps a `Dataset` object and provides an iterable to access the data. Its key responsibilities are:\n",
    "- **Batching:** This involves grouping individual samples into mini-batches.\n",
    "- **Shuffling:** This involves randomly shuffling the data every epoch to prevent the model from learning the order of samples. The data loading process is parallelized, preventing the CPU from becoming a bottleneck for the GPU.\n",
    "\n",
    "We can use the `DataLoader` with **both** our `CustomBinaryClassDataset` and the `ImageFolder` dataset.\n",
    "\n",
    "Linear and convolution layers in a neural network can work **with or without** the batch dimension \n",
    "- e.g., with shapes `[C, H, W]` or  `[B, C, H, W]`, where:\n",
    "    - `B`: `batch_size`\n",
    "    - `C`: `number of channels`\n",
    "    - `H`: `image height`\n",
    "    - `W`: `image width`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created successfully!\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8 #  the number of images in a batch (batch size)\n",
    "\n",
    "# --- Using your Custom Dataset ---\n",
    "custom_dataset = CustomBinaryClassDataset(dir_non_agri, dir_agri, transform=custom_transform)\n",
    "custom_loader = DataLoader(custom_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "# --- Using the ImageFolder Dataset ---\n",
    "imagefolder_loader = DataLoader(imagefolder_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "print(\"DataLoaders created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a batch\n",
    "\n",
    "Let's inspect a batch from our `DataLoader`. You can use `iter()` and `next()` to pull a single batch. This is exactly what a training loop does under the hood. Visualizing the images and their labels confirms that our entire pipeline (from path loading and transformation to batching) is working correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images batch shape (Custom loader): torch.Size([8, 3, 64, 64])\n",
      "Labels batch shape (Custom loader): torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Get one batch from the Custom loader\n",
    "images_custom, labels_custom = next(iter(custom_loader))\n",
    "\n",
    "print(f\"Images batch shape (Custom loader): {images_custom.shape}\") \n",
    "print(f\"Labels batch shape (Custom loader): {labels_custom.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to get the images and labels from the loader. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 4**: Get a batch of images and labels from the `imagefolder_loader` and print their shape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "images_inbuilt, labels_inbuilt = next(iter(imagefolder_loader))\n",
    "\n",
    "print(f\"Images batch shape (PyTorch in-built loader): {images_inbuilt.shape}\") \n",
    "print(f\"Labels batch shape (PyTorch in-built loader): {labels_inbuilt.shape}\")\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a function to display an image from the batch. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    \"\"\"Helper function to un-normalize and display an image\"\"\"\n",
    "    img = img / 2 + 0.5  # Un-normalize from [-1, 1] to [0, 1]\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0))) # Convert from C,H,W to H,W,C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, display the images in the PyTorch in-built loader batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 6))\n",
    "for i in range(BATCH_SIZE):\n",
    "    ax = plt.subplot(2, 4, i + 1)\n",
    "    imshow(images_inbuilt[i])\n",
    "    plt.title(f\"PyTorch_loader Label:{labels_inbuilt[i].item()}\")\n",
    "    plt.axis(\"off\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 5**: Display the images in the Custom loader batch\n",
    "Similar to the code cell above, display the images stored in `images_custom` generated using `custom_loader`. \n",
    "\n",
    "The title of the images should be **`Custom_loader Label: `** similar to the images seen in the above cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(BATCH_SIZE):\n",
    "    ax = plt.subplot(2, 4, i + 1)\n",
    "    imshow(images_custom[i])\n",
    "    plt.title(f\"Custom_loader Label:{labels_custom[i].item()}\")\n",
    "    plt.axis(\"off\")\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison and analysis\n",
    "\n",
    "In PyTorch, both methods lead to a `DataLoader`. The key difference lies in how the underlying `Dataset` is created.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature                  | Custom `Dataset`                                       | In-built `ImageFolder`     |\n",
    "|--------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Ease of use**          | Low. It requires writing a custom class with three specific methods. | High. It requires only a single, intuitive class instantiation. |\n",
    "| **Code complexity**      | High. You manually manage file paths, labels, and the loading logic inside `__getitem__`. | Low. The class handles all path and label discovery automatically.          |\n",
    "| **Flexibility**          | Very high. It is essential for non-standard directory structures, loading labels from a CSV/JSON, or complex loading logic (for example, from a database). | Moderate. It strictly requires the `root/class/image.ext` directory structure. |\n",
    "| **Performance**          | Potentially high. Performance depends on the efficiency of your `__getitem__` method. When wrapped in a `DataLoader` with `num_workers`, it can be highly performant. | High. `ImageFolder` is optimized and, when wrapped in a `DataLoader`, provides excellent performance out-of-the-box. |\n",
    "| **Recommended for**      | It is recommended for complex, non-standard datasets or when you need fine-grained control over every aspect of data loading. | It is recommended for almost all standard image classification tasks.        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed ntoebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This lab has demonstrated the power and elegance of PyTorch's data loading API. The key takeaway is the **separation of concerns**:\n",
    "- The `Dataset` class defines *what* the data is and *how to get a single item*.\n",
    "- The `DataLoader` class defines *how to iterate over* the data, handling batching, shuffling, and performance optimization.\n",
    "\n",
    "While building a custom `Dataset` is an excellent way to understand the underlying mechanics and is necessary for complex scenarios, `ImageFolder` provides a simple and robust solution for the vast majority of image classification problems. In either case, the `DataLoader` is the essential tool that makes the entire pipeline efficient and ready for training a deep learning model.\n",
    "\n",
    "You are now equipped with standard, production-ready methods for loading image data in PyTorch!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!--\n",
    "## Change Log\n",
    "\n",
    "'''|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "```\n",
    "```|---|---|---|---|\n",
    "```\n",
    "```| 2025-06-21  | 1.0  | Aman  |  Created the lab |\n",
    "```\n",
    "```| 2025-06-28  | 2.0  | Sangeeta |  ID review |\n",
    "```\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "017effb9f4acffb3b145a3533aac36aee79c9dfdee4ec6a43e504aca99352785"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
